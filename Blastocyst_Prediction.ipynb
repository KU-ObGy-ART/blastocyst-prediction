{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DijN8OHbeHo9"
      },
      "source": [
        "## Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdB-UIPBOih4",
        "outputId": "cf51127d-1baa-4519-a28b-f2d04a1a22bb"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "# Restart the runtime after downgrading tensorflow\n",
        "!pip install -q xgboost\n",
        "!pip install -q tensorflow==2.12 # Our NASNetLarge weight works with TensorFlow and Keras v2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qHDg5en2jwE",
        "outputId": "8a783b19-7c0d-429a-9d03-4e05e8a6b6ef"
      },
      "outputs": [],
      "source": [
        "# Download and unzip required files and sample images from Google Drive\n",
        "!pip install -q gdown\n",
        "!gdown --id 15Y_R_rNcRym61EPp_jAKqHxkNuXEHX7y\n",
        "!unzip -q Blastocyst_Prediction.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJGwJVJyaZsS"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "%matplotlib inline\n",
        "import os, glob, cv2\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tqdm.notebook import tqdm\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.nasnet import NASNetLarge\n",
        "import xgboost\n",
        "\n",
        "import sklearn\n",
        "import os\n",
        "import torch\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "annotation_label_dict = {0:'1cell',1:'1cell_PN',\n",
        "                         2:'2cell_G1',3:'2cell_G2',4:'2cell_G3',\n",
        "                         5:'3cell_G1',6:'3cell_G2',7:'3cell_G3',\n",
        "                         8:'4cell_G1',9:'4cell_G2',10:'4cell_G3',\n",
        "                         11:'5-7cell_G1',12:'5-7cell_G2',13:'5-7cell_G3',\n",
        "                         14:'8-cell_G1',15:'8-cell_G2',16:'8-cell_G3'}\n",
        "\n",
        "timelapse_min = 24\n",
        "timelapse_max = 64\n",
        "time_list = np.arange(timelapse_min, timelapse_max+15/60.0, 15/60.0)\n",
        "\n",
        "def convert_to_wide_format(df_long,time_name,label_name):\n",
        "    df_long[label_name] = df_long[label_name].astype(float)\n",
        "    df_long = df_long.drop_duplicates(['ID',time_name]).reset_index(drop=True)\n",
        "    df_wide = df_long[['ID',time_name,label_name]].set_index(['ID',time_name]).unstack().reset_index().T.reset_index().drop(columns=['level_0']).set_index(time_name).T.rename(columns={'':'ID'})\n",
        "    return df_wide\n",
        "\n",
        "# grade and cell stage list settings\n",
        "G1_label_list = [2,5,8,11,14]\n",
        "G2_label_list = [3,6,9,12,15]\n",
        "G3_label_list = [4,7,10,13,16]\n",
        "cell_2_8_list = G1_label_list+G2_label_list+G3_label_list\n",
        "cell_1_list = [0,1]\n",
        "cell_2_list = [G1_label_list[0],G2_label_list[0],G3_label_list[0]]\n",
        "cell_3_list = [G1_label_list[1],G2_label_list[1],G3_label_list[1]]\n",
        "cell_4_list = [G1_label_list[2],G2_label_list[2],G3_label_list[2]]\n",
        "cell_5_list = [G1_label_list[3],G2_label_list[3],G3_label_list[3]]\n",
        "cell_8_list = [G1_label_list[4],G2_label_list[4],G3_label_list[4]]\n",
        "\n",
        "def process_df_data(df_data,timelapse_min=24, timelapse_max=64, interval=15):\n",
        "    time_list = np.arange(timelapse_min, timelapse_max, interval/60.0)\n",
        "    df_data['ratio_1cell'] = df_data[time_list].isin(cell_1_list).sum(axis=1)/df_data[time_list].notna().sum(axis=1)\n",
        "    df_data['ratio_2cell'] = df_data[time_list].isin(cell_2_list).sum(axis=1)/df_data[time_list].notna().sum(axis=1)\n",
        "    df_data['ratio_3cell'] = df_data[time_list].isin(cell_3_list).sum(axis=1)/df_data[time_list].notna().sum(axis=1)\n",
        "    df_data['ratio_4cell'] = df_data[time_list].isin(cell_4_list).sum(axis=1)/df_data[time_list].notna().sum(axis=1)\n",
        "    df_data['ratio_5cell'] = df_data[time_list].isin(cell_5_list).sum(axis=1)/df_data[time_list].notna().sum(axis=1)\n",
        "    df_data['ratio_8cell'] = df_data[time_list].isin(cell_8_list).sum(axis=1)/df_data[time_list].notna().sum(axis=1)\n",
        "    df_data['ratio_G1'] = df_data[time_list].isin(G1_label_list).sum(axis=1)/df_data[time_list].isin(cell_2_8_list).sum(axis=1)\n",
        "    df_data['ratio_G2'] = df_data[time_list].isin(G2_label_list).sum(axis=1)/df_data[time_list].isin(cell_2_8_list).sum(axis=1)\n",
        "    df_data['ratio_G3'] = df_data[time_list].isin(G3_label_list).sum(axis=1)/df_data[time_list].isin(cell_2_8_list).sum(axis=1)\n",
        "\n",
        "    df_data['ratio_2cell_G1'] = 0\n",
        "    df_data['ratio_2cell_G2'] = 0\n",
        "    df_data['ratio_2cell_G3'] = 0\n",
        "    df_data['ratio_3cell_G1'] = 0\n",
        "    df_data['ratio_3cell_G2'] = 0\n",
        "    df_data['ratio_3cell_G3'] = 0\n",
        "    df_data['ratio_4cell_G1'] = 0\n",
        "    df_data['ratio_4cell_G2'] = 0\n",
        "    df_data['ratio_4cell_G3'] = 0\n",
        "    df_data['ratio_5cell_G1'] = 0\n",
        "    df_data['ratio_5cell_G2'] = 0\n",
        "    df_data['ratio_5cell_G3'] = 0\n",
        "    df_data['ratio_8cell_G1'] = 0\n",
        "    df_data['ratio_8cell_G2'] = 0\n",
        "    df_data['ratio_8cell_G3'] = 0\n",
        "\n",
        "    df_data.loc[df_data['ratio_2cell']!=0,'ratio_2cell_G1'] = df_data.loc[df_data['ratio_2cell']!=0,time_list].isin([G1_label_list[0]]).sum(axis=1)/df_data.loc[df_data['ratio_2cell']!=0,time_list].isin(cell_2_list).sum(axis=1)\n",
        "    df_data.loc[df_data['ratio_2cell']!=0,'ratio_2cell_G2'] = df_data.loc[df_data['ratio_2cell']!=0,time_list].isin([G2_label_list[0]]).sum(axis=1)/df_data.loc[df_data['ratio_2cell']!=0,time_list].isin(cell_2_list).sum(axis=1)\n",
        "    df_data.loc[df_data['ratio_2cell']!=0,'ratio_2cell_G3'] = df_data.loc[df_data['ratio_2cell']!=0,time_list].isin([G3_label_list[0]]).sum(axis=1)/df_data.loc[df_data['ratio_2cell']!=0,time_list].isin(cell_2_list).sum(axis=1)\n",
        "    df_data.loc[df_data['ratio_3cell']!=0,'ratio_3cell_G1'] = df_data.loc[df_data['ratio_3cell']!=0,time_list].isin([G1_label_list[1]]).sum(axis=1)/df_data.loc[df_data['ratio_3cell']!=0,time_list].isin(cell_3_list).sum(axis=1)\n",
        "    df_data.loc[df_data['ratio_3cell']!=0,'ratio_3cell_G2'] = df_data.loc[df_data['ratio_3cell']!=0,time_list].isin([G2_label_list[1]]).sum(axis=1)/df_data.loc[df_data['ratio_3cell']!=0,time_list].isin(cell_3_list).sum(axis=1)\n",
        "    df_data.loc[df_data['ratio_3cell']!=0,'ratio_3cell_G3'] = df_data.loc[df_data['ratio_3cell']!=0,time_list].isin([G3_label_list[1]]).sum(axis=1)/df_data.loc[df_data['ratio_3cell']!=0,time_list].isin(cell_3_list).sum(axis=1)\n",
        "    df_data.loc[df_data['ratio_4cell']!=0,'ratio_4cell_G1'] = df_data.loc[df_data['ratio_4cell']!=0,time_list].isin([G1_label_list[2]]).sum(axis=1)/df_data.loc[df_data['ratio_4cell']!=0,time_list].isin(cell_4_list).sum(axis=1)\n",
        "    df_data.loc[df_data['ratio_4cell']!=0,'ratio_4cell_G2'] = df_data.loc[df_data['ratio_4cell']!=0,time_list].isin([G2_label_list[2]]).sum(axis=1)/df_data.loc[df_data['ratio_4cell']!=0,time_list].isin(cell_4_list).sum(axis=1)\n",
        "    df_data.loc[df_data['ratio_4cell']!=0,'ratio_4cell_G3'] = df_data.loc[df_data['ratio_4cell']!=0,time_list].isin([G3_label_list[2]]).sum(axis=1)/df_data.loc[df_data['ratio_4cell']!=0,time_list].isin(cell_4_list).sum(axis=1)\n",
        "    df_data.loc[df_data['ratio_5cell']!=0,'ratio_5cell_G1'] = df_data.loc[df_data['ratio_5cell']!=0,time_list].isin([G1_label_list[3]]).sum(axis=1)/df_data.loc[df_data['ratio_5cell']!=0,time_list].isin(cell_5_list).sum(axis=1)\n",
        "    df_data.loc[df_data['ratio_5cell']!=0,'ratio_5cell_G2'] = df_data.loc[df_data['ratio_5cell']!=0,time_list].isin([G2_label_list[3]]).sum(axis=1)/df_data.loc[df_data['ratio_5cell']!=0,time_list].isin(cell_5_list).sum(axis=1)\n",
        "    df_data.loc[df_data['ratio_5cell']!=0,'ratio_5cell_G3'] = df_data.loc[df_data['ratio_5cell']!=0,time_list].isin([G3_label_list[3]]).sum(axis=1)/df_data.loc[df_data['ratio_5cell']!=0,time_list].isin(cell_5_list).sum(axis=1)\n",
        "    df_data.loc[df_data['ratio_8cell']!=0,'ratio_8cell_G1'] = df_data.loc[df_data['ratio_8cell']!=0,time_list].isin([G1_label_list[4]]).sum(axis=1)/df_data.loc[df_data['ratio_8cell']!=0,time_list].isin(cell_8_list).sum(axis=1)\n",
        "    df_data.loc[df_data['ratio_8cell']!=0,'ratio_8cell_G2'] = df_data.loc[df_data['ratio_8cell']!=0,time_list].isin([G2_label_list[4]]).sum(axis=1)/df_data.loc[df_data['ratio_8cell']!=0,time_list].isin(cell_8_list).sum(axis=1)\n",
        "    df_data.loc[df_data['ratio_8cell']!=0,'ratio_8cell_G3'] = df_data.loc[df_data['ratio_8cell']!=0,time_list].isin([G3_label_list[4]]).sum(axis=1)/df_data.loc[df_data['ratio_8cell']!=0,time_list].isin(cell_8_list).sum(axis=1)\n",
        "\n",
        "    df_data['grade_2cell'] = np.argmax(np.stack([df_data['ratio_2cell_G1'].values,df_data['ratio_2cell_G2'].values,df_data['ratio_2cell_G3'].values],axis=1),axis=1)+1\n",
        "    df_data['grade_3cell'] = np.argmax(np.stack([df_data['ratio_3cell_G1'].values,df_data['ratio_3cell_G2'].values,df_data['ratio_3cell_G3'].values],axis=1),axis=1)+1\n",
        "    df_data['grade_4cell'] = np.argmax(np.stack([df_data['ratio_4cell_G1'].values,df_data['ratio_4cell_G2'].values,df_data['ratio_4cell_G3'].values],axis=1),axis=1)+1\n",
        "    df_data['grade_5cell'] = np.argmax(np.stack([df_data['ratio_5cell_G1'].values,df_data['ratio_5cell_G2'].values,df_data['ratio_5cell_G3'].values],axis=1),axis=1)+1\n",
        "    df_data['grade_8cell'] = np.argmax(np.stack([df_data['ratio_8cell_G1'].values,df_data['ratio_8cell_G2'].values,df_data['ratio_8cell_G3'].values],axis=1),axis=1)+1\n",
        "\n",
        "    # Grade at each cell stage is substituted and unified.\n",
        "    df_data.loc[df_data['grade_2cell']==1,time_list] = df_data.loc[df_data['grade_2cell']==1,time_list].replace({2:4,3:4,4:4})\n",
        "    df_data.loc[df_data['grade_2cell']==2,time_list] = df_data.loc[df_data['grade_2cell']==2,time_list].replace({2:3,3:3,4:3})\n",
        "    df_data.loc[df_data['grade_2cell']==3,time_list] = df_data.loc[df_data['grade_2cell']==3,time_list].replace({2:2,3:2,4:2})\n",
        "    df_data.loc[df_data['grade_3cell']==1,time_list] = df_data.loc[df_data['grade_3cell']==1,time_list].replace({5:7,6:7,7:7})\n",
        "    df_data.loc[df_data['grade_3cell']==2,time_list] = df_data.loc[df_data['grade_3cell']==2,time_list].replace({5:6,6:6,7:6})\n",
        "    df_data.loc[df_data['grade_3cell']==3,time_list] = df_data.loc[df_data['grade_3cell']==3,time_list].replace({5:5,6:5,7:5})\n",
        "    df_data.loc[df_data['grade_4cell']==1,time_list] = df_data.loc[df_data['grade_4cell']==1,time_list].replace({8:10,9:10,10:10})\n",
        "    df_data.loc[df_data['grade_4cell']==2,time_list] = df_data.loc[df_data['grade_4cell']==2,time_list].replace({8:9,9:9,10:9})\n",
        "    df_data.loc[df_data['grade_4cell']==3,time_list] = df_data.loc[df_data['grade_4cell']==3,time_list].replace({8:8,9:8,10:8})\n",
        "    df_data.loc[df_data['grade_5cell']==1,time_list] = df_data.loc[df_data['grade_5cell']==1,time_list].replace({11:13,12:13,13:13})\n",
        "    df_data.loc[df_data['grade_5cell']==2,time_list] = df_data.loc[df_data['grade_5cell']==2,time_list].replace({11:12,12:12,13:12})\n",
        "    df_data.loc[df_data['grade_5cell']==3,time_list] = df_data.loc[df_data['grade_5cell']==3,time_list].replace({11:11,12:11,13:11})\n",
        "    df_data.loc[df_data['grade_8cell']==1,time_list] = df_data.loc[df_data['grade_8cell']==1,time_list].replace({14:16,15:16,16:16})\n",
        "    df_data.loc[df_data['grade_8cell']==2,time_list] = df_data.loc[df_data['grade_8cell']==2,time_list].replace({14:15,15:15,16:15})\n",
        "    df_data.loc[df_data['grade_8cell']==3,time_list] = df_data.loc[df_data['grade_8cell']==3,time_list].replace({14:14,15:14,16:14})\n",
        "    df_data.drop(columns=[f'ratio_{cell}cell_G{grade}' for cell in [2,3,4,5,8] for grade in [1,2,3]]+[f'grade_{cell}cell' for cell in [2,3,4,5,8]],inplace=True)\n",
        "    return df_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jli4NZYUAOTw"
      },
      "source": [
        "##Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "XQqmjbqhdOCH",
        "outputId": "7ccc3343-18c0-4007-d4fa-b79fc045912b"
      },
      "outputs": [],
      "source": [
        "# Preprocess the Images\n",
        "\n",
        "# File Paths Configuration\n",
        "# The CSV file includes the following columns: 'ID', 'age' (the age at egg retrieval), 'time (hpi)' (post-fertilization hour), and 'img_path' (original image file path).\n",
        "csv_path = '/content/sample_img_list.csv'\n",
        "original_image_dir = '/content/sample_images'\n",
        "preprocess_image_dir = '/content/sample_images_preprocessed'\n",
        "\n",
        "# Settings : Adjust to the time-lapse images for each incubator.\n",
        "\n",
        "extend_r = 15  # Pixels to extend the cropping area from the recognized area\n",
        "export_img_size = 331  # Exported image size (square)\n",
        "min_recognition_size = 150  # Minimum size of recognized objects\n",
        "max_recognition_size = 600  # Maximum size of recognized objects\n",
        "setting_list = [[10, 50, 50],[5, 50, 50], [4, 40, 40], [7, 50, 50]]  # Contour detection settings [kernel_size, canny_th1, canny_th2]\n",
        "level_adjustment = False  # Enable level_adjustment (set it to True) when the image is dark.\n",
        "\n",
        "# Start time for benchmarking\n",
        "start = datetime.now()\n",
        "\n",
        "# Create directories for saving processed images\n",
        "os.makedirs(preprocess_image_dir, exist_ok=True)\n",
        "\n",
        "# Load CSV\n",
        "df_process = pd.read_csv(csv_path)\n",
        "df_process['preprocess_img_path'] = df_process['img_path'].str.replace(original_image_dir, preprocess_image_dir)\n",
        "\n",
        "# Initialize execution status\n",
        "exe = []\n",
        "\n",
        "for i, row in df_process.iterrows():\n",
        "    path = row['img_path']\n",
        "    save_path = row['preprocess_img_path']\n",
        "    img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "\n",
        "    if img is None:\n",
        "        exe.append(0)\n",
        "        continue\n",
        "\n",
        "    # Preprocess image: grayscale conversion and level adjustment\n",
        "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    if level_adjustment:\n",
        "        add_grayscale = 255 - np.percentile(img_gray, 98)\n",
        "        img_gray = np.clip(img_gray + add_grayscale, 0, 255).astype(np.uint8)\n",
        "\n",
        "    # Contour detection and processing\n",
        "    radius, x, y = None, None, None\n",
        "    for kernel_size, canny_th1, canny_th2 in setting_list:\n",
        "        # Canny edge detection\n",
        "        edges = cv2.Canny(img_gray, canny_th1, canny_th2)\n",
        "        # Morphological operations\n",
        "        kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
        "        processed = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)\n",
        "        processed = cv2.morphologyEx(processed, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "        # Extract contours\n",
        "        contours, _ = cv2.findContours(processed, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        if not contours:\n",
        "            continue\n",
        "\n",
        "        # Sort contours by area and find the largest valid contour\n",
        "        contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
        "        for cnt in contours:\n",
        "            if len(cnt) > 5:\n",
        "                (x, y), (width, height), _ = cv2.fitEllipse(cnt)\n",
        "                radius = max(width, height) / 2\n",
        "                if min_recognition_size <= radius <= max_recognition_size:\n",
        "                    break\n",
        "                else:\n",
        "                    radius = None\n",
        "            else:\n",
        "                radius = None\n",
        "        if radius is not None:\n",
        "            break\n",
        "\n",
        "    if radius is None:\n",
        "        exe.append(0)\n",
        "        continue\n",
        "\n",
        "    # Extend the cropping radius\n",
        "    radius += extend_r\n",
        "    h, w = img_gray.shape\n",
        "    center = (int(x), int(y))\n",
        "    export_size = int(radius * 2)\n",
        "\n",
        "    # Ensure cropping area is within image bounds\n",
        "    if (center[1] - radius < 0 or center[1] + radius > h or\n",
        "        center[0] - radius < 0 or center[0] + radius > w):\n",
        "        exe.append(0)\n",
        "        continue\n",
        "\n",
        "    # Crop and resize the image\n",
        "    cropped_img = img_gray[\n",
        "        max(0, center[1] - export_size // 2):min(h, center[1] + export_size // 2),\n",
        "        max(0, center[0] - export_size // 2):min(w, center[0] + export_size // 2)\n",
        "    ]\n",
        "    resized_img = cv2.resize(cropped_img, (export_img_size, export_img_size))\n",
        "\n",
        "    # Create a circular mask\n",
        "    mask = np.zeros((export_img_size, export_img_size), dtype=np.uint8)\n",
        "    cv2.circle(mask, (export_img_size // 2, export_img_size // 2), export_img_size // 2, 255, -1)\n",
        "    masked_img = cv2.bitwise_and(resized_img, resized_img, mask=mask)\n",
        "\n",
        "    # Save the processed image\n",
        "    try:\n",
        "        cv2.imwrite(save_path, masked_img)\n",
        "        exe.append(1)\n",
        "    except Exception as e:\n",
        "        exe.append(0)\n",
        "        print(f\"Error saving image {save_path}: {e}\")\n",
        "\n",
        "# Record the end time and calculate processing time\n",
        "end = datetime.now()\n",
        "processing_time = end - start\n",
        "\n",
        "# Update the DataFrame with execution status\n",
        "df_process['execution'] = exe\n",
        "df_process.loc[df_process['execution'] == 0, 'preprocess_img_path'] = np.nan\n",
        "success_rate = np.mean(exe) * 100\n",
        "print(f\"Processed {len(df_process)} images with a success rate of {success_rate:.1f}%.\")\n",
        "print(f\"Processing time: {processing_time}\")\n",
        "\n",
        "# Display the DataFrame\n",
        "df_process.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JqBwzPyG8dfI",
        "outputId": "fd46d90f-3685-4fdb-ef3a-3c8b8981534c"
      },
      "outputs": [],
      "source": [
        "# Check the Preprocessed Images\n",
        "\n",
        "# Setting: Number of images per figure\n",
        "images_per_figure = 48\n",
        "rows, cols = 6, 8\n",
        "\n",
        "for start in range(0, len(df_process), images_per_figure):\n",
        "    end = min(start + images_per_figure, len(df_process))\n",
        "    chunk = df_process.iloc[start:end]\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(20, 15))\n",
        "    axes = axes.flatten()\n",
        "    for i, ax in enumerate(axes):\n",
        "        idx = start + i\n",
        "        if idx < len(df_process):\n",
        "            img_path = df_process.iloc[idx]['preprocess_img_path']\n",
        "            try:\n",
        "                img = Image.open(img_path).convert('RGB')\n",
        "                ax.imshow(img)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {img_path}: {e}\")\n",
        "                ax.text(0.5, 0.5, 'Error', ha='center', va='center', fontsize=12)\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EEf-kIkBD2w"
      },
      "source": [
        "## Embryo Image Auto-Annotation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "mkoinIJUB0Yo",
        "outputId": "90590404-8822-4f84-f875-dea5a03194aa"
      },
      "outputs": [],
      "source": [
        "# Embryo Image Auto-Annotation Using Fine-Tuned NASNet-A Large\n",
        "\n",
        "# Exclude unprocessed images\n",
        "df_process = df_process.dropna(subset=['preprocess_img_path']).reset_index(drop=True)\n",
        "\n",
        "# Make dataset from DataFrame (image file paths)\n",
        "pred_dataset = ImageDataGenerator(rescale=1.0/255).flow_from_dataframe(df_process, directory='', x_col='preprocess_img_path', target_size=(331, 331), color_mode='rgb', classes=None, class_mode='input', batch_size=32, shuffle=False, seed=None, save_to_dir=None, save_prefix='', save_format='png', subset=None, interpolation='nearest', drop_duplicates=False)\n",
        "\n",
        "# Load the Model and Fine-Tuned Weight\n",
        "model_path =  f'/content/NASNet_embryo_17class'\n",
        "model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "# Auto-Annotation Using Fine-Tuned NASNet-A Large\n",
        "features = model.predict(pred_dataset)\n",
        "df_process['annotation'] = np.argmax(features,axis=1)\n",
        "df_process['annotation_label'] = df_process['annotation'].map(annotation_label_dict)\n",
        "display(df_process)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m1tcpDNT5uH"
      },
      "source": [
        "## Blastocyst Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "RaQEEFim3aM4",
        "outputId": "03c32b3e-1d22-41fd-d53f-8f86dd8eb97c"
      },
      "outputs": [],
      "source": [
        "# Dataset Creation for XGBoost Predictions\n",
        "df_data = pd.merge(df_process[['ID','age']].drop_duplicates(),convert_to_wide_format(df_process,'time(hpi)','annotation'))\n",
        "df_data = process_df_data(df_data)\n",
        "display(df_data)\n",
        "\n",
        "# Loading configurations for prediction models\n",
        "df_config = pd.read_pickle('/content/df_config.pkl')\n",
        "\n",
        "# Prediction with XGBoost\n",
        "df_result = df_data[['ID','age',64.0]]\n",
        "for i in range(len(df_config)):\n",
        "    target_label = df_config['target_label'].values[i]\n",
        "    list_features = df_config['list_var'].values[i]\n",
        "    list_features = [float(x) if x.replace('.', '', 1).isdigit() else x for x in list_features]\n",
        "    xgb = XGBClassifier()\n",
        "    xgb.load_model(f'/content/xgb_{target_label}.model')\n",
        "    df_result[target_label] = xgb.predict_proba(df_data[list_features].astype(float))[:,1]\n",
        "df_result['annotation at 64hpi'] = df_result[64.0].map(annotation_label_dict)\n",
        "df_result  = df_result[['ID','age','annotation at 64hpi']+df_config['target_label'].values.tolist()]\n",
        "display(df_result)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
